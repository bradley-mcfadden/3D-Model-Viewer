\thechapter{Description}
\thesection{Lighting}

\begin{figure}[h]
    \centering
        \begin{subfigure}[b]{0.16\textwidth}
        \includegraphics[width=\textwidth]{img/Lighting/ambient.png}
        \caption{Ambient}
        \label{fig:ambient}
    \end{subfigure}
    ~
    \centering
    \begin{subfigure}[b]{0.16\textwidth}
        \includegraphics[width=\textwidth]{img/Lighting/diffuse.png}
        \caption{Diffuse}
        \label{fig:diffuse}
    \end{subfigure}
     ~
    \centering
    \begin{subfigure}[b]{0.16\textwidth}
        \includegraphics[width=\textwidth]{img/Lighting/specular.png}
        \caption{Specular}
        \label{fig:specular}
    \end{subfigure}
    ~
    \centering
    \begin{subfigure}[b]{0.16\textwidth}
        \includegraphics[width=\textwidth]{img/Lighting/combined.png}
        \caption{Combined}
        \label{fig:combined}
    \end{subfigure}
    \caption{Phong lighting applied to Engineer model}
    \label{fig:Lighting}
\end{figure}

The Phong lighting model is used, with specular, diffuse, and ambient light combined to give 
the model a realistic 3D shape in \autoref{fig:Lighting}. Specular light concentrates light in one direction, providing 
highlights, diffuse lighting scatters light equally, providing shadows, and ambient light provides 
an equal amount of light to all parts of the surface from every direction. 

\begin{figure}[h]
\centering
\begin{subfigure}[b]{0.2\textwidth}
    \includegraphics[width=\textwidth]{img/Lighting/Rim.png}
    \caption{Engineer}
    \label{fig:Rim}
\end{subfigure}
~
\hspace{36pt}
~
\begin{subfigure}[b]{0.22\textwidth}
    \includegraphics[width=\textwidth]{img/Lighting/RimSkull.png}
    \caption{Skull}
    \label{fig:RimSkull}
\end{subfigure}
\caption{Rim lighting applied to models}
\label{fig:RimLighting}
\end{figure}

Rim lighting, shown in \autoref{fig:RimLighting}, is also added to provide additional highlights and increase the cartoon appearance 
of the render as specular highlights are not that effective, likely as properties of the model's 
material is consistent throughout it's entire texture always with a shininess of 0.5. It works by 
highlighting edges of the model near to perpendicular to the light, by calculating the dot product 
of the light and fragment position and checking for it to be between 0.00 and 0.16. This implementation 
does not work well on  models that do not have smooth curves, and the distance and position of the 
light must also be changed for different models for optimal results. 

\begin{lstlisting}[ caption={Fragment shader code for Phong and rim lighting},
    label={listing-phong-lighting}, 
    language=C]
d = dot(ptLightNorm, ptNorm);

// add Phong lighting
ambientPortion = ambientColour;
diffusePortion = diffuseColour * d;
halfway = normalize(lightNorm - fragmentNorm);
s = max(dot(fragmentNorm, halfway), 0.0) ^ 0.5;
specularPortion = specularColour  * s;

// add rim lighting
if (d > 0 && d < 0.16)
	rimPortion = vec4(0.4, 0.4, 0.4, 1);

lighting = ambientPortion + diffusePortion
+ specularPortion + rimPortion;
\end{lstlisting}

Lighting is implemented in the fragment shader, shown in \autoref{listing-phong-lighting}, rather than in the vertex shader, decreasing 
efficiency as it must be calculated for every fragment rather than the relatively small number of 
vertices, but providing more precise results. Our implementation of Phong lighting is as described 
by Angel in \cite{texbook}, and the model can be displayed with either a directional or a point light 
source. A code snippet of our implementation is provided in \autoref{listing-phong-lighting}.

\newpage 
% referring to the colour discretization process
\thesection{Cel shading}
Cel shading refers to an NPR technique where to better simulate a cartoon art style, the tones of 
an image are broken down into bands. The colour palette of a scene is reduced to a handful of colours.
The technique simulates how an artist might draw an image, where only a few shades are used, and few
separate colours are used.

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.15\textwidth}
        \includegraphics[width=\textwidth]{img/cel-shading-n1.png}
        \caption{$n = 1$}
        \label{fig:cel-shading-n1}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.15\textwidth}
        \includegraphics[width=\textwidth]{img/cel-shading-n2.png}
        \caption{$n = 2$}
        \label{fig:cel-shading-n2}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.15\textwidth}
        \includegraphics[width=\textwidth]{img/cel-shading-n4.png}
        \caption{$n = 4$}
        \label{fig:cel-shading-n4}
    \end{subfigure}

    \begin{subfigure}[b]{0.15\textwidth}
        \includegraphics[width=\textwidth]{img/cel-shading-n8.png}
        \caption{$n = 8$}
        \label{fig:cel-shading-n8}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.15\textwidth}
        \includegraphics[width=\textwidth]{img/cel-shading-n16.png}
        \caption{$n = 16$}
        \label{fig:cel-shading-n16}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.15\textwidth}
        \includegraphics[width=\textwidth]{img/cel-shading-none.png}
        \caption{None}
        \label{fig:cel-shading-none}
    \end{subfigure}
    \caption{Comparison of number of cel shading bands}
    \label{fig:cel-band-comparison}
\end{figure}


To perform cel shading, a number of discrete bands n must be chosen. Higher values for n produce more 
bands of colour, which leads to less harsh contrast in the image, but fewer bands create more contrast,
and therefore a more dramatic effect. To get the final colour of a pixel, the RGB values are divided by
$n+1$, which returns the index of the band. Then, the index is multiplied by the size of the band, which
produces the final colour. Examples of cel shading for a particular model are shown in 
\autoref{fig:cel-band-comparison}. 

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.15\textwidth}
        \includegraphics[width=\textwidth]{img/cel-shading-highpoly-n18.png}
        \caption{$n = 18$}
        \label{fig:cel-shading-high-poly-n18}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.15\textwidth}
        \includegraphics[width=\textwidth]{img/cel-shading-highpoly-none.png}
        \caption{None}
        \label{fig:cel-shading-high-poly-none}
    \end{subfigure}
    \caption{Cel shading with a high polygon count model}
    \label{fig:cel-shading-high-poly}
\end{figure}

One can notice sharp edges on the model. This is implementation specific, and happens because the 
colour passed to the fragment shader is interpolated between vertices by OpenGL. Normally, this creates 
a smooth effect and saves processing power, but when performing cel shading, it can destroy the effect, 
so it is turned off. Unfortunately, doing so creates jagged lines in models with a low polygon count, 
but the effect is  not as noticeable for higher polygon count models, such as in 
\autoref{fig:cel-shading-high-poly}. 

\newpage 

% referring to the lines between the model and the background
\thesection{Contours}
Contours are a common technique used in none photorealistic rendering that involves highlighting 
the boundaries and shape of an image with outlines. This technique makes a model appear like a drawing 
and helps show depth and other details in areas where color normally would.

The contours are constructed by adding lines to the model relative to the camera, creating outlines 
similar to a sketch of the model. This is achieved by drawing lines around distinct parts of the model 
and on areas that have a sharp change in angle.

\begin{figure}[h]
    \centering
        \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[height=100pt]{img/ModelA.png}
        \caption{No Offset}
        \label{fig:NOffset}
    \end{subfigure}
    ~
        \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[height=100pt]{img/ModelB.png}
        \caption{Offset}
        \label{fig:Offset}
    \end{subfigure}
    ~
        \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[height=100pt]{img/ModelC.png}
        \caption{Close-Up}
        \label{fig:ZOffset}
    \end{subfigure}
    \caption{Contours using GL\_Line and offset}
    \label{fig:ZOffset}
    \end{figure}

Our first implementation only gave some basic suggestive contours and was made using OpenGL's built-in. 
PolygonMode system. More specifically, it was done by first rendering the model in GL\_Line mode, as 
seen in the none offset image \autoref{fig:NOffset}. Next, filled polygons are rendered and offset towards 
the camera. Making it so that the only visible lines would be the Contours. Creating the middle image \autoref{fig:Offset}.

\begin{figure}[h]
    \centering
    \includegraphics[height=100pt]{img/ModelD.png}
    \caption{Contours using the stencil buffer}
    \label{fig:Stencil}
\end{figure}

Next, we moved on to our second implementation using the stencil buffer instead of 
offsetting the outline. To do this, we needed to render in the reverse order of our first attempt. As 
the stencil buffer first needs to be told where it can to write to. To do this, the stencil buffer 
first needs to be enabled and cleared. Next, the buffer is set to record where on the screen information 
is generated to a matrix before rendering the base model.

After this, we move on to rendering the outline using the inverse of the generated matrix. The matrix is
set to read-only so new images can only be rendered to areas not written to in the first pass-through.
Finally the image is outlined using GL\_Line mode to create the model seen in \autoref{fig:Stencil}. Stencil testing produces a much less detailed contour, but without any of the 
artefacts of our first algorithm.

\newpage
\thesection{Textures}

\begin{figure}[h]
    \centering
        \begin{subfigure}[b]{0.14\textwidth}
        \includegraphics[width=\textwidth]{img/textures/Original.png}
        \caption{Engineer}
        \label{fig:Original}
    \end{subfigure}
    ~
    \centering
    \begin{subfigure}[b]{0.14\textwidth}
        \includegraphics[width=\textwidth]{img/textures/CelShadeTexture6.png}
        \caption{6 Levels}
        \label{fig:CelShadeTexture4}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.14\textwidth}
        \includegraphics[width=\textwidth]{img/textures/CelShadeTexture8.png}
        \caption{8 Levels}
        \label{fig:CelShadeTexture8}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.16\textwidth}
        \includegraphics[width=\textwidth]{img/textures/OriginalSkull.png}
        \caption{Skull}
        \label{fig:OriginalSkull}
	\end{subfigure}
    ~
    \begin{subfigure}[b]{0.16\textwidth}
        \includegraphics[width=\textwidth]{img/textures/CelShadeTexture8Skull.png}
        \caption{8 Levels}
        \label{fig:CelShadeTexture8Skull}
    \end{subfigure}
    %\begin{subfigure}[b]{0.2\textwidth}
     %   \includegraphics[width=\textwidth]{img/textures/TextureLightingCelShade.png}
    %    \caption{Final}
    %    \label{fig:TextureLightingCelShade}
    %\end{subfigure}
    \caption{Cel shaded textures}
    \label{fig:TexturesEngineer}
\end{figure}

Texture coordinates are loaded from the initial .obj file and stored with each vertex, and can then be used 
both to render the model with its original texture or to replace the texture with another image. In the fragment 
shader textures are used to map fragments to the color at the fragment's texture coordinate interpolated from the 
stored vertex coordinate. The color mapped by the coordinate in the model's textures can then be further modified 
through cel shading so photo realistic versions are rendered in a consistent toon style, shown in \autoref{fig:TexturesEngineer}.

\begin{figure}[h]
    \centering
        \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{img/textures/space.png}
        \caption{Initial Texture}
        \label{fig:Space}
    \end{subfigure}
~
\hspace{24pt}
~
    \centering
    \begin{subfigure}[b]{0.16\textwidth}
        \includegraphics[width=\textwidth]{img/Combined/SpaceSkullNoContour.png}
        \caption{Skull}
        \label{fig:TextureReplacementSkull}
    \end{subfigure}
~
\hspace{24pt}
~
    \centering
    \begin{subfigure}[b]{0.13\textwidth}
        \includegraphics[width=\textwidth]{img/Combined/SpaceEngineerNoContour.png}
        \caption{Engineer}
        \label{fig:TextureReplacement}
    \end{subfigure}
    \caption{Replaced textures with cel shaded lighting}
    \label{fig:TexturesReplacement}
\end{figure}

Using a photo-realistic image transformed through cel shading as the new texture, shown in \autoref{fig:TexturesReplacement} can produce interesting 
renders with the same process used for the original textures. 

\newpage 
% probably could add a section about lighting the model
\thesection{System overview}
The described features were implemented into a real-time rendering system. The system itself 
was written in OpenGL3. Specifically, we used LWJGL's implementation of OpenGL. We took advantage
of the vertex and fragment shaders that OpenGL3 requires to be used. The use of vertex and fragment
shaders allows us to implement most algorithms on the GPU, which saves CPU cycles and increases the
performance of the application.

% I'm going to include a flow chart or something here to have a visual aid
\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{img/system-overview.png}
    \caption{An overview of stages in the system}
    \label{fig-sys-overview}
\end{figure}

Essentially, our system is a real-time renderer for 3D models that handles lighting, arbitrary
textures, and allows shading effects to be toggled on or off for comparison. A camera is supported
so that the model can be examined from a variety of viewing angles. A high-level overview of the
system is presented in \autoref{fig-sys-overview}.

% model_converter | Conversion stage
The first stage of our system takes in .obj files and reformats parts of the file in order to make
it easier to parse. This was implemented as a separate Python program, since it needs only be done 
once per model.
% WavefrontParser, MtlParser | Parsing stage
The second stage of the system parses a .obj file to produce a prototype of a model that our system
is later able to render. In doing so, the vertices, vertex normals, and texture coordinates from the
file are packed into the prototype model. Additionally, the indices that use this data to define
faces of the model are also read. At this stage, textures are also read in for the model from the
.mtl file referenced by the  .obj file. One caveat is that imported models should be triangulated,
as our system has no procedure in place to triangulate polygons.
% Model, setupBuffers | Initialization

The next stage of the system packs the model's data into a format that the vertex shader can 
understand. An strided array of nine floating point values per vertex is created. The first three
values each stride represent the vertex data, the next three are the vertex normal, and the final 
three contain texture coordinates. These values are passed as attributes to the vertex shader.
The buffer only ever has to be calculated once per model, so this stage does not repeat. 
% Model, .vs, .fs | Rendering
Every frame, each model is rendered. The render proceeds by passing light and camera parameters to 
the vertex shader, along with the vertex, vertex normal and texture coordinates from the previous 
stage. Two rendering stages happen for our program, the outline render loop, and the model render
loop. These stages are shown visually in \autoref{fig-render-overview}.

The outline render loop draws only the back faces of the model in a black wireframe. Edges
are drawn at a size of 5 pixels instead of the default 1 pixel, so they stick out from the model.
Furthermore, the polygons of the model are draw slightly offset to further accentuate the outlines.
One issue with this approach to drawing outlines is that the size of the outlines is constant no 
matter the distance to the model, so far away objects end up with much more noticeable outlines
than closer models.

% a figure of the wireframe would be cool here
\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{img/rendering-overview.png}
    \caption{An overview of stages in the rendering step}
    \label{fig-render-overview}
\end{figure}

The model render loop handles the lighting and the cel shading of the object, and the its work is
done in the vertex and frament shaders. The steps are detailed in \autoref{fig-render-overview}.

The vertex shader is the entry point for the OpenGL pipeline, so its main role is to forward the 
parameters that the fragment shader uses, and to specify the position of each vertex sent to it. 
Additionally, the vertex shader calculates the lighting vector relative to the viewing angle.

The fragment shader does the bulk of the work of our application. In the fragment shader, two main
procedures happen. First, the colour of the lighting for the fragment is found. This is a multi-step
process that involves applying diffuse lighting, rim lighting, and reducing extreme light and dark 
values in order to preserve the colour of the model. Afterward, cel shading is applied to discretize
the colours of the model. 

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.2\textwidth}
        \includegraphics[width=\textwidth]{img/teapot-wireframe.png}
        \caption{Outline}
        \label{fig:outline-render}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.2\textwidth}
        \includegraphics[width=\textwidth]{img/teapot-outline-noshade.png}
        \caption{Unshaded}
        \label{fig:model-noshade}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.2\textwidth}
        \includegraphics[width=\textwidth]{img/teapot-model.png}
        \caption{Shading}
        \label{fig:model-render}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.2\textwidth}
        \includegraphics[width=\textwidth]{img/teapot-complete.png}
        \caption{Final}
        \label{fig:complete-render}
    \end{subfigure}
    \caption{Comparison of output at each rendering stage}
    \label{fig:teapot-comparison}
\end{figure}

\autoref{fig:teapot-comparison} shows the rendering process at three stages, drawing the outline,
drawing the inner model, and the combined final result. One drawback to this approach of drawing
outlines is that the rendering pipeline is used twice per model. However, it may not be quite as 
intensive since only back faces are drawn, and the wireframe of the model is not filled.
